import h5py
import os
import pickle
from matplotlib import pyplot as plt
import numpy as np
import multiprocessing as mp
from tqdm import tqdm
import torch

# ==========================
# Step 1: å¹¶è¡Œå¤„ç†åŸå§‹æ•°æ®
# ==========================
def process_single_file(k):
    """
    å¤„ç†å•ä¸ª .mat æ–‡ä»¶ï¼Œå¹¶è¡Œæå–æ•°æ®
    """
    fs = 125   # é‡‡æ ·ç‡
    t = 10     # PPG ç‰‡æ®µé•¿åº¦
    dt = 5     # ç‰‡æ®µé—´éš”
    samples_in_episode = round(fs * t)
    d_samples = round(fs * dt)

    f = h5py.File(os.path.join('raw_data', f'Part_{k}.mat'), 'r')
    ky = f'Part_{k}'
    output_dir = 'processed_data'
    os.makedirs(output_dir, exist_ok=True)

    for i in range(len(f[ky])):
        subject_id = f"{k}_{i}"  # å”¯ä¸€ Subject ID
        signal = np.array(f[f[ky][i][0]][:,0], dtype=np.float32)  # è¯»å– PPG ä¿¡å·
        bp = np.array(f[f[ky][i][0]][:,1], dtype=np.float32)  # è¯»å– ABP ä¿¡å·
        if len(signal)==0 or len(bp)==0:
            continue

        output_str = '10s,SBP,DBP,Subject_ID\n'
        for j in range(0, len(signal) - samples_in_episode, d_samples):
            sbp = np.max(bp[j:j+samples_in_episode])  
            dbp = np.min(bp[j:j+samples_in_episode])
            output_str += f"{j},{sbp},{dbp},{subject_id}\n"

        with open(os.path.join(output_dir, f'Part_{k}_{i}.csv'), 'w') as fp:
            fp.write(output_str)

def process_data():
    """
    å¹¶è¡Œå¤„ç†æ‰€æœ‰ Part_k.mat æ–‡ä»¶
    """
    os.makedirs('processed_data', exist_ok=True)
    num_workers = min(mp.cpu_count(), 4)  
    with mp.Pool(num_workers) as pool:
        pool.map(process_single_file, range(1, 5)) 

# ==========================
# Step 2: å¹¶è¡Œä¸‹é‡‡æ ·
# ==========================
def downsample_data(minThresh=2500, ratio=0.25):
    """
    é‡‡ç”¨ SBP å’Œ DBP ç›´æ–¹å›¾è¿›è¡Œä¸‹é‡‡æ ·ï¼Œé¿å…è¿‡é‡‡æ ·é«˜é¢‘æ®µã€‚
    """
    files = next(os.walk('processed_data'))[2]  # è·å–æ‰€æœ‰ csv æ–‡ä»¶

    sbps_dict, dbps_dict = {}, {}  # å­˜å‚¨ SBP å’Œ DBP çš„æ•°æ®ç´¢å¼•
    sbps_cnt, dbps_cnt = {}, {}  # å­˜å‚¨ SBP å’Œ DBP çš„å‡ºç°æ¬¡æ•°

    candidates = []  # è®°å½•æœ€ç»ˆé€‰æ‹©çš„ episodes

    # âœ… 1. è¯»å–æ‰€æœ‰æ•°æ®ï¼Œå¹¶æŒ‰ SBP/DBP è¿›è¡Œåˆ†ç±»
    for fl in tqdm(files, desc="è¯»å–æ•°æ®æ–‡ä»¶"):
        lines = open(os.path.join('processed_data', fl), 'r').read().split('\n')[1:-1]
        for line in lines:
            values = line.split(',')
            file_no = int(fl.split('_')[1])  # æ–‡ä»¶ç¼–å·
            record_no = int(fl.split('.')[0].split('_')[2])  # è®°å½•ç¼–å·
            episode_st = int(values[0])  # ç‰‡æ®µèµ·å§‹ç‚¹
            sbp = int(float(values[1]))  # SBP
            dbp = int(float(values[2]))  # DBP

            if sbp not in sbps_dict:
                sbps_dict[sbp] = []
                sbps_cnt[sbp] = 0
            sbps_dict[sbp].append((file_no, record_no, episode_st))
            sbps_cnt[sbp] += 1

            if dbp not in dbps_dict:
                dbps_dict[dbp] = []
                dbps_cnt[dbp] = 0
            dbps_dict[dbp].append((file_no, record_no, episode_st, sbp))
            dbps_cnt[dbp] += 1

    # âœ… 2. æŒ‰ DBP è¿›è¡Œä¸‹é‡‡æ ·
    dbp_keys = sorted(dbps_dict.keys())  # è·å–æ‰€æœ‰ DBP å€¼ï¼Œå¹¶æ’åº
    dbps_taken = {}  # è®°å½•å·²é€‰æ‹©çš„ DBP æ•°æ®
    sbps_taken = {}  # è®°å½•å·²é€‰æ‹©çš„ SBP æ•°æ®

    for dbp in tqdm(dbp_keys, desc="æŒ‰ DBP é€‰æ‹©æ•°æ®"):
        cnt = min(int(dbps_cnt[dbp] * ratio), minThresh)  # è®¡ç®—éœ€è¦é€‰å–çš„æ•°é‡
        if cnt == 0:
            continue  # é¿å… 0 ä¸ªæ•°æ®çš„æƒ…å†µ

        indices = np.random.choice(len(dbps_dict[dbp]), size=cnt, replace=False)  # é€‰æ‹©ç´¢å¼•
        for ind in indices:
            file_no, record_no, episode_st, sbp = dbps_dict[dbp][ind]
            candidates.append((file_no, record_no, episode_st))
            dbps_taken[dbp] = dbps_taken.get(dbp, 0) + 1  # è®°å½•å·²é€‰æ•°é‡
            sbps_taken[sbp] = sbps_taken.get(sbp, 0) + 1

    # âœ… 3. æŒ‰ SBP è¿›è¡Œé¢å¤–çš„ä¸‹é‡‡æ ·ï¼ˆé¿å… DBP é€‰ä¸­çš„æ•°æ®è¢«é‡å¤é€‰æ‹©ï¼‰
    sbp_keys = sorted(sbps_dict.keys())  # è·å–æ‰€æœ‰ SBP å€¼ï¼Œå¹¶æ’åº

    for sbp in tqdm(sbp_keys, desc="æŒ‰ SBP é€‰æ‹©æ•°æ®"):
        cnt = min(int(sbps_cnt[sbp] * ratio), minThresh)  # è®¡ç®—éœ€è¦é€‰å–çš„æ•°é‡
        cnt -= sbps_taken.get(sbp, 0)  # å‡å»å·²ä» DBP é‡‡æ ·çš„æ•°é‡
        cnt = max(cnt, 0)  # é¿å…è´Ÿæ•°
        cnt = min(cnt, len(sbps_dict[sbp]))  # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨æ ·æœ¬æ•°
        if cnt == 0:
            continue  # é¿å… 0 ä¸ªæ•°æ®çš„æƒ…å†µ

        indices = np.random.choice(len(sbps_dict[sbp]), size=cnt, replace=False)  # é€‰æ‹©ç´¢å¼•
        for ind in indices:
            file_no, record_no, episode_st = sbps_dict[sbp][ind]
            candidates.append((file_no, record_no, episode_st))
            sbps_taken[sbp] = sbps_taken.get(sbp, 0) + 1  # è®°å½•å·²é€‰æ•°é‡

    # âœ… 4. ä¿å­˜ä¸‹é‡‡æ ·åçš„æ•°æ®
    print(f"ğŸ¯ å…±é€‰æ‹© {len(candidates)} ä¸ª episodes")
    pickle.dump(candidates, open(os.path.join('output','candidates.p'), 'wb'))

    # âœ… 5. ç»˜åˆ¶ç›´æ–¹å›¾ï¼ŒæŸ¥çœ‹ä¸‹é‡‡æ ·åˆ†å¸ƒ
    sbp_counts = [sbps_taken.get(sbp, 0) for sbp in sbp_keys]
    dbp_counts = [dbps_taken.get(dbp, 0) for dbp in dbp_keys]

    plt.figure(figsize=(10, 6))
    plt.subplot(2, 1, 1)
    plt.bar(sbp_keys, sbp_counts, color='b')
    plt.title('sbp')

    plt.subplot(2, 1, 2)
    plt.bar(dbp_keys, dbp_counts, color='r')
    plt.title('dbp')

    plt.savefig('imgs/downsample.png')


# ==========================
# Step 3: å¹¶è¡Œæå– episodes
# ==========================
def extract_single_episode(file_no, record_no, episode_st):
    """
    å¤„ç†å•ä¸ª episode å¹¶ç”¨ PyTorch åŠ é€Ÿè®¡ç®—
    """
    fs = 125
    t = 10
    samples_in_episode = round(fs * t)

    # âœ… åªåœ¨å½“å‰è¿›ç¨‹æ‰“å¼€ h5py.Fileï¼Œé¿å…è·¨è¿›ç¨‹é”™è¯¯
    with h5py.File(f'./raw_data/Part_{file_no}.mat', 'r') as f:
        ky = f'Part_{file_no}'

        # âœ… è¯»å– PPG å’Œ ABP æ•°æ®
        try:
            ppg = torch.tensor(f[f[ky][record_no][0]][episode_st:episode_st+samples_in_episode, 0], dtype=torch.float32)
            abp = torch.tensor(f[f[ky][record_no][0]][episode_st:episode_st+samples_in_episode, 1], dtype=torch.float32)
        except KeyError:
            print(f"âš ï¸ è®°å½• {file_no}_{record_no} ä¸å­˜åœ¨ï¼Œè·³è¿‡ episode {episode_st}")
            return

        # âœ… æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if len(ppg) < samples_in_episode or len(abp) < samples_in_episode:
            print(f"âš ï¸ è·³è¿‡ {file_no}_{record_no}_{episode_st}, æ•°æ®é•¿åº¦ä¸è¶³ {samples_in_episode}!")
            return

    # âœ… ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
    os.makedirs('ppgs', exist_ok=True)
    os.makedirs('abps', exist_ok=True)

    # âœ… ä¿å­˜ PPG å’Œ ABP
    pickle.dump(ppg, open(os.path.join('ppgs', f'{file_no}_{record_no}_{episode_st}.p'), 'wb'))
    pickle.dump(abp, open(os.path.join('abps', f'{file_no}_{record_no}_{episode_st}.p'), 'wb'))



def extract_episodes():
    """
    å¹¶è¡Œæå– episodes
    """
    os.makedirs('ppgs', exist_ok=True)
    os.makedirs('abps', exist_ok=True)

    # âœ… åŠ è½½ä¸‹é‡‡æ ·åçš„ candidates
    downsampled_candidates = pickle.load(open('candidates.p', 'rb'))

    # âœ… é‡‡ç”¨ starmap ä¼ å¤šä¸ªå‚æ•°
    num_workers = min(mp.cpu_count(), 4)
    with mp.Pool(num_workers) as pool:
        pool.starmap(extract_single_episode, downsampled_candidates)

    print(f"âœ… æå–å®Œæˆï¼Œä¿å­˜ {len(downsampled_candidates)} æ¡ episodes")

# ==========================
# Step 4: åˆå¹¶ episodes
# ==========================
def process_episode(file_info):
    """
    è¯»å–å•ä¸ª episode å¹¶è¿”å› subject_id, abp, ppg
    """
    fl = file_info
    subject_id = "_".join(fl.split("_")[:2])  # âœ… è§£æ `{file_no}_{record_no}`

    # è¯»å–æ•°æ®
    abp = pickle.load(open(os.path.join('abps', fl), 'rb'))
    ppg = pickle.load(open(os.path.join('ppgs', fl), 'rb'))

    return subject_id, np.array(abp, dtype=np.float32), np.array(ppg, dtype=np.float32)


def merge_episodes():
    """
    å¹¶è¡Œåˆå¹¶ episodes å¹¶ä¿å­˜ä¸º HDF5 æ–‡ä»¶
    """
    os.makedirs('data', exist_ok=True)

    abp_files = sorted(next(os.walk('abps'))[2])  # è¯»å– ABP æ–‡ä»¶
    num_workers = min(mp.cpu_count(), 8)  # âœ… é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°ï¼Œé˜²æ­¢ IO è¿‡è½½

    # âœ… è¯»å–æ‰€æœ‰æ•°æ®å¹¶è¡Œå¤„ç†
    with mp.Pool(num_workers) as pool:
        results = list(tqdm(pool.imap(process_episode, abp_files, chunksize=10), 
                            total=len(abp_files), desc="Processing Episodes"))

    # âœ… ä½¿ç”¨ HDF5 å­˜å‚¨æ•°æ®
    with h5py.File(os.path.join('data', 'data_with_subjects.hdf5'), 'w', libver='latest', swmr=True) as f:
        subject_groups = {}  # å­˜å‚¨å·²åˆ›å»ºçš„ subject ç»„

        for subject_id, abp, ppg in tqdm(results, desc="Writing to HDF5"):
            if subject_id not in subject_groups:
                subject_groups[subject_id] = f.create_group(subject_id)  # âœ… æŒ‰ `{file_no}_{record_no}` ç»„ç»‡

            # âœ… è®¡ç®—å½“å‰ subject_id çš„ episode æ•°é‡
            ep_id = str(len(subject_groups[subject_id].keys()) // 2)  # ä¸¤ä¸ª dataset ä»£è¡¨ä¸€ä¸ª episode
            subject_groups[subject_id].create_dataset(f"abp_{ep_id}", data=abp)
            subject_groups[subject_id].create_dataset(f"ppg_{ep_id}", data=ppg)

    print(f"âœ… å¹¶è¡Œåˆå¹¶å®Œæˆï¼Œä¿å­˜ {len(abp_files)} æ¡ episodes åˆ° 'data_with_subjects.hdf5' ğŸš€")


# ==========================
# Step 5: è¿è¡Œä¸»æµç¨‹
# ==========================
def main():
    # process_data()  # å¹¶è¡Œæ•°æ®å¤„ç†
    # downsample_data()  # å¹¶è¡Œä¸‹é‡‡æ ·
    # extract_episodes()  # å¹¶è¡Œæå–
    merge_episodes()  # å­˜å‚¨æœ€ç»ˆæ•°æ®

if __name__ == '__main__':
    main()
